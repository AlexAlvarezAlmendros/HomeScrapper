# HomeScraper - Multi-Portal Property Scraper

Web scraper **escalable** para m√∫ltiples portales inmobiliarios, con enfoque especial en detectar y filtrar anuncios de **particulares** (propietarios directos) vs inmobiliarias/profesionales.

## üåü Portales Soportados

‚úÖ **Idealista** - Portal l√≠der en Espa√±a  
‚úÖ **Fotocasa** - Portal de clasificados inmobiliarios  
üîú **F√°cilmente extensible** a nuevos portales (Pisos.com, Habitaclia, etc.)

## üöÄ Caracter√≠sticas Principales

- **üîå Arquitectura Multi-Portal**: Sistema escalable basado en Factory Pattern
- **üéØ Filtrado de dos etapas**: Identifica particulares en listado y verifica en detalle
- **‚ôæÔ∏è Detecci√≥n autom√°tica de p√°ginas**: Procesa todas las p√°ginas disponibles
- **üìä Extracci√≥n completa de datos**: T√≠tulo, precio, ubicaci√≥n, habitaciones, m¬≤, descripci√≥n
- **üõ°Ô∏è M√©todo CDP**: Conexi√≥n a Chrome en modo debug para evitar detecci√≥n
- **üîÑ Rotaci√≥n de IP**: Soporte para cambio autom√°tico de VPN (NordVPN, Windscribe, etc.)
- **ü§ñ Anti-detecci√≥n**: Delays aleatorios, pausas inteligentes, manejo de captchas

## üìÅ Estructura del Proyecto

### Scrapers Core

- **`HomeScraper.py`** ‚≠ê - Script principal con men√∫ interactivo multi-portal
- **`base_scraper.py`** - Clase base abstracta con funcionalidad com√∫n
- **`idealista_scraper.py`** - Scraper espec√≠fico para Idealista
- **`fotocasa_scraper.py`** - Scraper espec√≠fico para Fotocasa
- **`scraper_factory.py`** - Factory para gestionar portales de forma escalable

### Archivos Legacy

- **`HomeScraperIdealista.py`** - Versi√≥n antigua solo para Idealista (mantener por compatibilidad)

### Archivos de Configuraci√≥n

- **`start_chrome_debug.bat`**: Inicia Chrome en modo debugging
- **`requirements_advanced.txt`**: Dependencias del proyecto
- **`profile_4931/`**: Perfil de Chrome para debugging

## üõ†Ô∏è Instalaci√≥n

### üêß Linux (Ubuntu/Debian/Xubuntu)

1. **Clonar el repositorio**
```bash
git clone https://github.com/AlexAlvarezAlmendros/HomeScrapper.git
cd HomeScrapper
```

2. **Instalar Python y pip (si no los tienes)**
```bash
sudo apt update
sudo apt install python3 python3-pip python3-venv python-is-python3
```

3. **Instalar Chrome o Chromium**
```bash
# Opci√≥n A: Chromium (recomendado)
sudo apt install chromium-browser

# Opci√≥n B: Google Chrome
wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
sudo dpkg -i google-chrome-stable_current_amd64.deb
sudo apt -f install
```

4. **Crear y activar entorno virtual**
```bash
python -m venv .venv
source .venv/bin/activate
```

5. **Instalar dependencias**
```bash
pip install -r requirements_advanced.txt
```

6. **Dar permisos a los scripts**
```bash
chmod +x start_chrome_debug.sh start_scraper.sh
```

### ü™ü Windows

1. **Clonar el repositorio**
```bash
git clone https://github.com/AlexAlvarezAlmendros/HomeScrapper.git
cd HomeScrapper
```

2. **Crear entorno virtual**
```bash
python -m venv .venv
```

3. **Activar entorno virtual**
```powershell
# Windows PowerShell
.venv\Scripts\Activate.ps1

# Windows CMD
.venv\Scripts\activate.bat
```

4. **Instalar dependencias**
```bash
pip install -r requirements_advanced.txt
```

## üìñ Uso

### Script Principal: HomeScraper.py (Recomendado)

Este es el script multi-portal con men√∫ interactivo.

#### Paso 1: Iniciar Chrome en modo debugging
```bash
.\start_chrome_debug.bat
```

#### Paso 2: Ejecutar el scraper
```bash
python HomeScraper.py
```

#### Paso 3: Seguir el men√∫ interactivo

**Selecci√≥n de Portal:**
```
Portales disponibles:
  1. Idealista
  2. Fotocasa

Elige un portal (1-2):
```

**Configuraci√≥n:**
```
[?] ¬øActivar modo DEBUG? (s/n)
    > Muestra informaci√≥n detallada del proceso de detecci√≥n

[?] ¬øActivar ROTACI√ìN DE IP? (s/n)
    > Cambia de IP autom√°ticamente o manualmente cada N peticiones
    > Soporte para NordVPN, ExpressVPN, ProtonVPN, Surfshark, Windscribe

[?] OPCIONES:
    1. Navegar autom√°ticamente a la URL y scrapear
    2. Ya estoy en la p√°gina, scrapear directamente

[?] ¬øCu√°ntas p√°ginas quieres scrapear?
    > Enter = TODAS las p√°ginas disponibles
    > O especifica un n√∫mero (ej: 3)
```

### Uso del Script Legacy (Solo Idealista)

Si solo necesitas Idealista, puedes usar el script original:

```bash
python HomeScraperIdealista.py
```

## üéØ C√≥mo Funciona el Filtrado de Particulares

### Paso 1: Filtrado en el Listado
Analiza cada anuncio en la p√°gina de resultados y descarta aquellos que tienen el elemento:
```html
<picture class="logo-branding">
```
Este elemento solo aparece en anuncios de inmobiliarias.

### Paso 2: Verificaci√≥n en Detalle
Para cada posible particular, accede a la p√°gina de detalle y busca:
```html
<div class="professional-name">
  <div class="name">Particular</div>
</div>
```

### Paso 3: Extracci√≥n de Datos
Si es confirmado como particular, extrae:
- ‚úÖ T√≠tulo de la vivienda
- ‚úÖ Precio
- ‚úÖ Ubicaci√≥n/Direcci√≥n
- ‚úÖ N√∫mero de habitaciones
- ‚úÖ Metros cuadrados
- ‚úÖ Descripci√≥n completa
- ‚úÖ Tel√©fono de contacto (hace click en "Ver tel√©fono")

## üìä Formato de Salida

Los resultados se guardan en formato JSON con timestamp:

```json
{
  "timestamp": "2025-11-14T16:05:49",
  "url": "https://www.idealista.com/...",
  "total": 45,
  "particulares": 12,
  "inmobiliarias": 33,
  "viviendas": {
    "todas": [...],
    "solo_particulares": [
      {
        "titulo": "Casa de pueblo en venta",
        "precio": "195.000 ‚Ç¨",
        "ubicacion": "Major-ag.bellmun, 7",
        "habitaciones": "5 hab.",
        "metros": "263 m¬≤",
        "url": "https://www.idealista.com/...",
        "descripcion": "...",
        "anunciante": "Particular",
        "telefono": "936 17 16 04",
        "fecha_scraping": "2025-11-14T16:05:49"
      }
    ],
    "solo_inmobiliarias": [...]
  }
}
```

Nombre del archivo: `viviendas_<portal>_YYYYMMDD_HHMMSS.json`

## üîß A√±adir Nuevos Portales

La arquitectura es totalmente escalable. Para a√±adir un nuevo portal:

### Paso 1: Crear el scraper espec√≠fico

```python
# nuevo_portal_scraper.py
from base_scraper import BaseScraper, Vivienda
from bs4 import BeautifulSoup

class NuevoPortalScraper(BaseScraper):
    
    def get_portal_name(self) -> str:
        return "NuevoPortal"
    
    def get_search_url(self) -> str:
        return "https://www.nuevoportal.com/buscar/..."
    
    def es_particular(self, html_texto: str) -> tuple[bool, str]:
        # Implementar l√≥gica espec√≠fica del portal
        pass
    
    def extraer_vivienda(self, elemento):
        # Implementar extracci√≥n espec√≠fica del portal
        pass
    
    def scrapear_pagina(self):
        # Implementar scraping de p√°gina
        pass
    
    def scrapear_con_filtrado(self, paginas=None):
        # Implementar m√©todo principal
        pass
```

### Paso 2: Registrar en la factory

```python
# scraper_factory.py
from nuevo_portal_scraper import NuevoPortalScraper

class ScraperFactory:
    _scrapers: Dict[str, Type[BaseScraper]] = {
        'idealista': IdealistaScraper,
        'fotocasa': FotocasaScraper,
        'nuevoportal': NuevoPortalScraper,  # ‚Üê A√±adir aqu√≠
    }
```

**¬°Listo!** El nuevo portal aparecer√° autom√°ticamente en el men√∫.

## üîç Detecci√≥n Autom√°tica de P√°ginas

El scraper detecta autom√°ticamente cu√°ndo ha llegado a la √∫ltima p√°gina:

- Busca botones de "siguiente p√°gina" en el DOM
- Detecta redirecciones o URLs repetidas
- Para autom√°ticamente cuando no hay m√°s resultados

## üõ°Ô∏è Anti-Detecci√≥n y Rotaci√≥n de IP

### T√©cnicas Anti-Detecci√≥n:

1. **Chrome DevTools Protocol (CDP)**
   - Conexi√≥n a Chrome ya abierto, no automatizaci√≥n detectable

2. **Delays aleatorios**
   - Entre p√°ginas: 3-7 segundos
   - Entre detalles: 2-5 segundos
   - Pausas largas cada 10 peticiones: 15-30 segundos

3. **Rotaci√≥n de IP autom√°tica**
   - Soporte para VPNs: NordVPN, ExpressVPN, ProtonVPN, Surfshark, Windscribe
   - Cambio autom√°tico cada N peticiones (configurable)
   - Modo manual con pausas para cambio manual

4. **Detecci√≥n y manejo de captchas**
   - Detecta captchas de DataDome autom√°ticamente
   - Pausa el scraper para resoluci√≥n manual
   - Contin√∫a autom√°ticamente despu√©s

## ‚ö†Ô∏è Consideraciones

- **Uso responsable**: No hacer scraping masivo que sobrecargue servidores
- **Terms of Service**: Revisa los t√©rminos de uso de cada portal
- **Rate limiting**: El scraper incluye pausas inteligentes
- **Datos personales**: Los datos extra√≠dos deben usarse responsablemente
- **IP bans**: Usa rotaci√≥n de IP si planeas hacer scraping extensivo

## üêõ Troubleshooting

### Error: "No se pudo conectar a Chrome"
**Soluci√≥n**: Ejecuta `start_chrome_debug.bat` primero y espera a que Chrome se abra

### Error: "Portal no disponible"
**Soluci√≥n**: Verifica que el portal est√© en la lista con `ScraperFactory.get_available_portals()`

### Captcha detectado constantemente
**Soluci√≥n**: 
- Activa rotaci√≥n de IP autom√°tica
- Aumenta los delays en `base_scraper.py`
- Reduce el n√∫mero de p√°ginas por sesi√≥n

### No se encuentran art√≠culos
**Soluci√≥n**: 
- Verifica que est√°s en una p√°gina de resultados del portal
- Los selectores CSS pueden haber cambiado, actualiza el scraper espec√≠fico
- Activa modo debug para ver qu√© est√° pasando

### Selectores CSS no funcionan
**Soluci√≥n**:
- Los portales cambian su HTML frecuentemente
- Inspecciona el HTML con DevTools de Chrome
- Actualiza los selectores en el archivo `<portal>_scraper.py` correspondiente

## üìù Licencia

Ver archivo [LICENSE](LICENSE)

## üë§ Autor

Alex Alvarez Almendros
- GitHub: [@AlexAlvarezAlmendros](https://github.com/AlexAlvarezAlmendros)

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Por favor:
1. Fork del proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit de tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ‚≠ê Agradecimientos

- Selenium WebDriver
- BeautifulSoup4
- undetected-chromedriver

---

**Nota**: Este proyecto es solo para fines educativos. √ösalo de manera responsable y respeta los t√©rminos de servicio de los sitios web.