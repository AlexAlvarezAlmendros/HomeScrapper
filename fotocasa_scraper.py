"""
Scraper espec√≠fico para Fotocasa
Hereda de BaseScraper e implementa l√≥gica espec√≠fica del portal

NOTA: Fotocasa permite extraer TODO desde el listado, no hace falta entrar al detalle
"""

import re
import time
import random
from typing import List, Optional
from datetime import datetime
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By

from base_scraper import BaseScraper, Vivienda


class FotocasaScraper(BaseScraper):
    """Scraper espec√≠fico para el portal Fotocasa"""
    
    def __init__(self, modo_debug=False, usar_rotacion_ip=False, vpn_provider=None, search_url=None):
        super().__init__(modo_debug, usar_rotacion_ip, vpn_provider)
        self.search_url = search_url or "https://www.fotocasa.es/es/comprar/viviendas/barcelona/capital/todas-las-zonas/l"
    
    def get_portal_name(self) -> str:
        return "Fotocasa"
    
    def get_search_url(self) -> str:
        return self.search_url
    
    def es_particular(self, html_texto: str) -> tuple[bool, str]:
        """
        Detecta si es particular usando se√±ales espec√≠ficas de Fotocasa
        En Fotocasa es muy f√°cil: buscar la imagen particular_user_icon.svg o el texto "Anunciante particular"
        """
        texto = html_texto.lower()
        
        # Se√±al 1: Imagen espec√≠fica de particular
        if 'particular_user_icon.svg' in texto:
            if self.modo_debug:
                print(f"      [DEBUG] ‚úì Detectado por icono de particular")
            return True, "Particular"
        
        # Se√±al 2: Texto expl√≠cito
        if 'anunciante  particular' in texto or 'anunciante particular' in texto:
            if self.modo_debug:
                print(f"      [DEBUG] ‚úì Detectado por texto 'Anunciante particular'")
            return True, "Particular"
        
        # Si no tiene ninguna se√±al de particular, es profesional
        if self.modo_debug:
            print(f"      [DEBUG] ‚úó No se encontraron se√±ales de particular")
        return False, "Profesional"
    
    def extraer_vivienda(self, articulo) -> Optional[Vivienda]:
        """Extrae datos de un art√≠culo de Fotocasa directamente desde el listado"""
        try:
            # Verificar primero si es particular
            es_part, tipo_anunciante = self.es_particular(str(articulo))
            
            # Si no es particular, saltar
            if not es_part:
                return None
            
            # T√≠tulo y URL - Buscar el enlace principal
            link = articulo.find('a', {'data-panot-component': 'link-box-link'})
            if not link:
                # Buscar enlace alternativo
                link = articulo.find('a', href=lambda x: x and '/d' in x)
            
            if not link:
                return None
            
            # Extraer t√≠tulo
            titulo_elem = link.find('strong')
            if titulo_elem:
                titulo_completo = link.get_text(strip=True)
                titulo = titulo_completo
            else:
                titulo = link.get_text(strip=True)
            
            # URL
            url = link.get('href', '')
            if url and not url.startswith('http'):
                url = f"https://www.fotocasa.es{url}"
            
            # Precio - buscar en el div con text-display-3
            precio_elem = articulo.find('div', class_='text-display-3')
            if not precio_elem:
                precio_elem = articulo.find('span', string=lambda x: x and '‚Ç¨' in str(x))
            precio = precio_elem.get_text(strip=True) if precio_elem else "N/A"
            
            # Ubicaci√≥n - puede estar en varios lugares
            ubicacion = "N/A"
            # Buscar en el t√≠tulo despu√©s del tipo de vivienda
            if 'en ' in titulo:
                try:
                    ubicacion = titulo.split('en ', 1)[1].strip()
                except:
                    pass
            
            # Caracter√≠sticas (habitaciones, ba√±os, metros) - buscar en la lista ul
            habitaciones = None
            metros = None
            banos = None
            
            caracteristicas_ul = articulo.find('ul', class_=lambda x: x and 'text-body-1' in x)
            if caracteristicas_ul:
                items = caracteristicas_ul.find_all('li', class_='inline')
                for item in items:
                    texto = item.get_text(strip=True).lower()
                    if 'hab' in texto:
                        habitaciones = texto.replace('¬∑', '').strip()
                    elif 'ba√±o' in texto:
                        banos = texto.replace('¬∑', '').strip()
                    elif 'm¬≤' in texto or 'm2' in texto:
                        metros = texto.replace('¬∑', '').strip()
            
            # Tel√©fono - buscar el enlace tel:
            telefono = None
            tel_link = articulo.find('a', href=lambda x: x and x.startswith('tel:'))
            if tel_link:
                telefono = tel_link.get('href').replace('tel:', '').strip()
                if self.modo_debug:
                    print(f"      [DEBUG] Tel√©fono encontrado: {telefono}")
            
            # Descripci√≥n oculta (puede estar en un <p class="hidden">)
            descripcion = None
            desc_elem = articulo.find('p', class_='hidden')
            if desc_elem:
                descripcion = desc_elem.get_text(strip=True)
            
            # Construir ubicaci√≥n m√°s completa si tenemos m√°s info
            if habitaciones or metros:
                detalles = f"{habitaciones or ''} {metros or ''}".strip()
            else:
                detalles = ""
            
            return Vivienda(
                titulo=titulo,
                precio=precio,
                ubicacion=ubicacion,
                habitaciones=habitaciones,
                metros=metros,
                url=url,
                descripcion=descripcion,
                anunciante="Particular",
                fecha_scraping=datetime.now().isoformat(),
                portal="Fotocasa",
                telefono=telefono  # Fotocasa tiene tel√©fono en listado
            )
            
        except Exception as e:
            if self.modo_debug:
                print(f"[DEBUG] Error extrayendo vivienda: {e}")
            return None
    
    def scrapear_pagina(self) -> List[Vivienda]:
        """Scrapea los anuncios de particulares de la p√°gina actual"""
        viviendas = []
        
        try:
            # Esperar a que cargue el contenido
            time.sleep(random.uniform(2, 4))
            
            # Verificar si hay captcha
            if self.detectar_captcha():
                print("    ‚ö†Ô∏è  Captcha detectado. Esperando resoluci√≥n manual...")
                input("    Presiona Enter cuando hayas resuelto el captcha...")
            
            # Obtener el HTML de la p√°gina
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # Buscar art√≠culos de anuncios
            # Fotocasa usa <article> con atributos data-*
            articulos = soup.find_all('article', attrs={'data-testid': lambda x: x and 'listing-card' in str(x)})
            if not articulos:
                # Intentar selector alternativo
                articulos = soup.find_all('article', class_=lambda x: x and '@container' in str(x))
            if not articulos:
                # Otro intento
                articulos = soup.find_all('article', class_='re-Card')
            if not articulos:
                articulos = soup.find_all('div', class_='re-Card')
            if not articulos:
                # Selector m√°s general
                articulos = soup.find_all('article')
            
            print(f"    üìã Encontrados {len(articulos)} anuncios en la p√°gina")
            
            for i, articulo in enumerate(articulos, 1):
                if self.modo_debug:
                    print(f"      [DEBUG] Procesando anuncio {i}/{len(articulos)}")
                
                vivienda = self.extraer_vivienda(articulo)
                if vivienda:
                    viviendas.append(vivienda)
                    if self.modo_debug:
                        print(f"      ‚úÖ PARTICULAR: {vivienda.titulo[:50]}...")
                else:
                    if self.modo_debug:
                        print(f"      ‚è≠Ô∏è  Anuncio descartado (no es particular o error)")
                
                # Peque√±o delay entre anuncios para parecer humano
                if i % 5 == 0:
                    time.sleep(random.uniform(0.5, 1.5))
            
            print(f"    ‚úÖ {len(viviendas)} viviendas de particulares encontradas en esta p√°gina\n")
            
        except Exception as e:
            print(f"    ‚ùå Error scrapeando p√°gina: {e}")
            if self.modo_debug:
                import traceback
                traceback.print_exc()
        
        return viviendas
    
    def filtrar_listado_particulares(self, paginas=None):
        """
        Filtra anuncios de particulares en Fotocasa.
        A diferencia de Idealista, NO necesita visitar detalle ya que todo est√° en el listado.
        """
        print("\n[FOTOCASA: FILTRADO DE PARTICULARES EN LISTADO]")
        print("="*70)
        
        paginas_procesadas = 0
        todas_viviendas = []
        continuar = True
        
        # Obtener URL base y par√°metros
        url_base = self.driver.current_url.split('?')[0]
        parametros = '?' + self.driver.current_url.split('?')[1] if '?' in self.driver.current_url else ''
        
        # Limpiar URL base de paginaci√≥n existente (quitar /2, /3, etc.)
        url_base = re.sub(r'/\d+$', '', url_base)
        
        # Asegurar que termina en /l
        if not url_base.endswith('/l'):
            if url_base.endswith('/'):
                url_base += 'l'
            else:
                url_base += '/l'
        
        if self.modo_debug:
            print(f"[DEBUG] URL base: {url_base}")
            print(f"[DEBUG] Par√°metros: {parametros}")
        
        while continuar:
            paginas_procesadas += 1
            
            if paginas is not None and paginas_procesadas > paginas:
                print(f"\n[*] Alcanzado l√≠mite de {paginas} p√°ginas")
                break
            
            print(f"\n--- P√°gina {paginas_procesadas} ---")
            
            # Construir URL de la p√°gina actual
            if paginas_procesadas == 1:
                url_pagina = url_base + parametros
            else:
                # Formato: /l/2, /l/3, etc.
                url_pagina = f"{url_base}/{paginas_procesadas}{parametros}"
            
            if self.modo_debug:
                print(f"[DEBUG] URL p√°gina {paginas_procesadas}: {url_pagina}")
            
            # Navegar a la p√°gina
            try:
                self._navegar_con_reintentos(url_pagina)
                time.sleep(random.uniform(2, 4))
                self.incrementar_contador_peticiones()
            except Exception as e:
                print(f"‚ùå Error navegando a p√°gina {paginas_procesadas}: {e}")
                break
            
            # Verificar si llegamos a una p√°gina sin resultados
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            no_results = soup.find('h3', class_='re-SearchNoResults-title')
            if no_results and 'ooops' in no_results.text.lower():
                print(f"\n‚úÖ Detectado final del listado (p√°gina sin resultados)")
                break
            
            # scrapear_pagina ya filtra y retorna solo particulares
            viviendas = self.scrapear_pagina()
            
            # Si no hay viviendas, probablemente llegamos al final
            if not viviendas and paginas_procesadas > 1:
                print(f"\n‚úÖ No se encontraron m√°s viviendas. Final del listado.")
                break
            
            todas_viviendas.extend(viviendas)
            
            print(f"[*] Total acumulado de particulares: {len(todas_viviendas)}")
            
            # Delay entre p√°ginas
            self.delay_aleatorio('pagina')
        
        print(f"\n[RESUMEN FINAL]")
        print(f"  P√°ginas procesadas: {paginas_procesadas}")
        print(f"  Total particulares encontrados: {len(todas_viviendas)}")
        print("="*70)
        
        return todas_viviendas
    
    def scrapear_con_filtrado(self, paginas=None):
        """M√©todo principal de scraping con filtrado en listado"""
        return self.filtrar_listado_particulares(paginas)
