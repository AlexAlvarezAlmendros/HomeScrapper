"""
HomeScraper - Scraper Multi-Portal de Viviendas
Soporta m√∫ltiples portales inmobiliarios de forma escalable
"""

import json
import os
import time
import random
from datetime import datetime
from scraper_factory import ScraperFactory
from base_scraper import PETICIONES_ANTES_CAMBIO_IP


def cargar_config():
    """Carga la configuraci√≥n de URLs desde config.json"""
    config_path = os.path.join(os.path.dirname(__file__), 'config.json')
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"[ERROR] No se encontr√≥ el archivo config.json")
        return None
    except json.JSONDecodeError as e:
        print(f"[ERROR] Error parseando config.json: {e}")
        return None


def seleccionar_url(portal, config):
    """Permite al usuario seleccionar una URL del portal.
    
    Returns:
        - Un dict {'url': ..., 'nombre': ...} para una sola URL
        - Una lista de dicts para modo batch (todas)
        - None si hay error
    """
    if portal not in config:
        print(f"[ERROR] Portal '{portal}' no encontrado en config.json")
        return None
    
    urls_disponibles = config[portal].get('urls', [])
    
    if not urls_disponibles:
        print(f"[ERROR] No hay URLs configuradas para {portal}")
        return None
    
    if len(urls_disponibles) == 1:
        url_data = urls_disponibles[0]
        print(f"\n[*] URL: {url_data['nombre']}")
        print(f"    {url_data.get('descripcion', '')}")
        return {'url': url_data['url'], 'nombre': url_data['nombre']}
    
    # Mostrar men√∫ de selecci√≥n
    print(f"\n{'='*70}")
    print(f"SELECCI√ìN DE ZONA - {portal.upper()}")
    print('='*70)
    print("\nZonas disponibles:")
    
    for idx, url_data in enumerate(urls_disponibles, 1):
        print(f"  {idx}. {url_data['nombre']}")
        if url_data.get('descripcion'):
            print(f"     ‚Üí {url_data['descripcion']}")
    
    print(f"\n  0. TODAS las zonas (modo batch autom√°tico)")
    
    print()
    while True:
        seleccion = input(f"Elige una zona (0-{len(urls_disponibles)}, 0 = todas): ").strip()
        
        if seleccion == '0' or seleccion.lower() == 'todas':
            print(f"\n[OK] Modo BATCH: se procesar√°n las {len(urls_disponibles)} zonas secuencialmente")
            return [{'url': u['url'], 'nombre': u['nombre']} for u in urls_disponibles]
        
        try:
            idx = int(seleccion) - 1
            if 0 <= idx < len(urls_disponibles):
                url_data = urls_disponibles[idx]
                print(f"\n[OK] Zona seleccionada: {url_data['nombre']}")
                return {'url': url_data['url'], 'nombre': url_data['nombre']}
            else:
                print(f"[!] Por favor, elige un n√∫mero entre 0 y {len(urls_disponibles)}")
        except ValueError:
            print("[!] Por favor, introduce un n√∫mero v√°lido")


def scrapear_idealista_batch(urls_list, debug, usar_rotacion, vpn_provider, num_paginas):
    """Procesa todas las URLs de Idealista secuencialmente via CDP."""
    from idealista_scraper import IdealistaScraper
    
    scraper = IdealistaScraper(
        modo_debug=debug,
        usar_rotacion_ip=usar_rotacion,
        vpn_provider=vpn_provider
    )
    
    if not scraper.conectar_chrome():
        return
    
    total = len(urls_list)
    for i, item in enumerate(urls_list, 1):
        url = item['url']
        nombre = item['nombre']
        
        print(f"\n\n{'#'*70}")
        print(f"  [{i}/{total}] PROCESANDO: {nombre}")
        print(f"  üîó {url[:80]}...")
        print(f"{'#'*70}")
        
        # Configurar URL del scraper
        scraper.search_url = url
        
        # Navegar a la URL
        print(f"\n[*] Navegando a {nombre}...")
        scraper.navegar_a_url()
        
        # Scrapear con filtrado (usa JSON persistente por ubicaci√≥n)
        viviendas = scraper.scrapear_con_filtrado(num_paginas, ubicacion=nombre)
        
        if viviendas:
            # Guardar en JSON persistente por ubicaci√≥n
            filename = IdealistaScraper._obtener_ruta_json_persistente(nombre)
            scraper.guardar(viviendas, filename, ubicacion=nombre, url_scrapeada=url)
            scraper.mostrar_resumen(viviendas)
        else:
            print(f"\n‚ö†Ô∏è  No se encontraron viviendas nuevas de particulares en {nombre}")
        
        if i < total:
            delay = random.uniform(8, 15)
            print(f"\n‚è≥ Esperando {delay:.0f}s antes de la siguiente zona...")
            time.sleep(delay)
    
    print(f"\n\n{'='*70}")
    print(f"  ‚úÖ BATCH COMPLETADO: {total} zonas procesadas")
    print(f"{'='*70}")


def scrapear_fotocasa_batch(urls_list, debug, num_paginas):
    """Procesa todas las URLs de Fotocasa secuencialmente via Playwright."""
    from fotocasa_scraper_firefox import FotocasaScraperFirefox
    
    scraper = FotocasaScraperFirefox(modo_debug=debug)
    
    if not scraper.iniciar_navegador():
        return
    
    total = len(urls_list)
    try:
        for i, item in enumerate(urls_list, 1):
            url = FotocasaScraperFirefox._asegurar_orden_fecha_fotocasa(item['url'])
            nombre = item['nombre']
            
            print(f"\n\n{'#'*70}")
            print(f"  [{i}/{total}] PROCESANDO: {nombre}")
            print(f"  üìÖ Ordenado por fecha de publicaci√≥n (m√°s recientes primero)")
            print(f"{'#'*70}")
            
            viviendas = scraper.scrapear(url, num_paginas, ubicacion=nombre)
            
            if viviendas:
                scraper.guardar_resultados(viviendas, ubicacion=nombre, url_scrapeada=url)
            else:
                print(f"\n‚ö†Ô∏è  No se encontraron viviendas nuevas de particulares en {nombre}")
            
            if i < total:
                delay = random.uniform(5, 10)
                print(f"\n‚è≥ Esperando {delay:.0f}s antes de la siguiente zona...")
                time.sleep(delay)
    finally:
        scraper.cerrar_navegador()
    
    print(f"\n\n{'='*70}")
    print(f"  ‚úÖ BATCH COMPLETADO: {total} zonas procesadas")
    print(f"{'='*70}")


def main():
    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë        HOME SCRAPER - MULTI PORTAL                   ‚ïë
    ‚ïë   Scraping de viviendas de particulares             ‚ïë
    ‚ïë   M√©todo CDP (Chrome DevTools Protocol)             ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    print("\n[!] INSTRUCCIONES RAPIDAS:")
    print("    1. Si NO has abierto Chrome con debugging:")
    print("       -> Ejecuta: start_chrome_debug.bat")
    print("       -> Espera a que Chrome se abra")
    print()
    print("    2. Si YA tienes Chrome abierto con debugging:")
    print("       -> Contin√∫a presionando Enter")
    print()
    
    input("Presiona Enter para continuar...")
    
    # Cargar configuraci√≥n
    config = cargar_config()
    if not config:
        return
    
    # ============== SELECCI√ìN DE PORTAL ==============
    print("\n" + "="*70)
    print("SELECCI√ìN DE PORTAL")
    print("="*70)
    
    portales = ScraperFactory.get_available_portals()
    
    print("\nPortales disponibles:")
    for idx, portal in enumerate(portales, 1):
        info = ScraperFactory.get_portal_info(portal)
        num_urls = len(config.get(portal, {}).get('urls', []))
        print(f"  {idx}. {info['name']} ({num_urls} zona(s) configurada(s))")
    
    print()
    portal_seleccionado = None
    while not portal_seleccionado:
        seleccion = input(f"Elige un portal (1-{len(portales)}): ").strip()
        
        try:
            idx = int(seleccion) - 1
            if 0 <= idx < len(portales):
                portal_seleccionado = portales[idx]
            else:
                print(f"[!] Por favor, elige un n√∫mero entre 1 y {len(portales)}")
        except ValueError:
            print("[!] Por favor, introduce un n√∫mero v√°lido")
    
    info_portal = ScraperFactory.get_portal_info(portal_seleccionado)
    print(f"\n[OK] Portal seleccionado: {info_portal['name']}")
    
    # ============== SELECCI√ìN DE URL ==============
    seleccion_url = seleccionar_url(portal_seleccionado, config)
    if not seleccion_url:
        return
    
    is_batch = isinstance(seleccion_url, list)
    
    # ============== CONFIGURACI√ìN COM√öN ==============
    
    # Modo debug
    print("\n[?] ¬øActivar modo DEBUG?")
    print("    (Mostrar√° c√≥mo se detecta cada particular)")
    debug = input("    s/n (Enter = no): ").strip().lower() == 's'
    
    # N√∫mero de p√°ginas
    print("\n[?] ¬øCu√°ntas p√°ginas quieres scrapear por zona?")
    print("    (Deja vac√≠o o escribe 'todas' para procesar todas las p√°ginas)")
    num_paginas_input = input("    N√∫mero (Enter = todas): ").strip().lower()
    
    if num_paginas_input == '' or num_paginas_input == 'todas' or num_paginas_input == 'all':
        num_paginas = None
        print("\n[*] Modo: TODAS LAS P√ÅGINAS (hasta detectar el final)")
    else:
        try:
            num_paginas = int(num_paginas_input)
            print(f"\n[*] Modo: {num_paginas} p√°gina(s) por zona")
        except:
            num_paginas = None
            print("\n[*] Valor no v√°lido, usando modo: TODAS LAS P√ÅGINAS")
    
    # ============== FOTOCASA: ANTI-DETECCI√ìN ==============
    if portal_seleccionado == 'fotocasa':
        print("\n" + "="*70)
        print("‚ö†Ô∏è  AVISO: Fotocasa tiene detecci√≥n anti-bot muy agresiva")
        print("="*70)
        print("\n[?] ¬øQu√© m√©todo quieres usar?")
        print("    1. Playwright Chromium (anti-detecci√≥n) - RECOMENDADO")
        print("       ‚Üí Abre su propio navegador")
        print("       ‚Üí No necesita Chrome en modo debugging")
        print("       ‚Üí Mayor probabilidad de √©xito")
        print("")
        print("    2. M√©todo CDP (Chrome debugging)")
        print("       ‚Üí Usa Chrome ya abierto")
        print("       ‚Üí Puede ser bloqueado f√°cilmente")
        print("")
        
        metodo = input("Elige m√©todo (1 o 2, Enter = 1): ").strip()
        
        if metodo != "2":
            print("\n[*] Usando Playwright Chromium...")
            
            if is_batch:
                scrapear_fotocasa_batch(seleccion_url, debug, num_paginas)
            else:
                scrapear_fotocasa_batch([seleccion_url], debug, num_paginas)
            
            print("\n‚úÖ Scraping completado")
            input("\nPresiona Enter para salir...")
            return
    
    # ============== IDEALISTA / FOTOCASA CDP ==============
    
    # Rotaci√≥n de IP (solo para CDP)
    print("\n[?] ¬øActivar ROTACI√ìN DE IP?")
    print("    (Te avisar√° cada cierto tiempo para cambiar IP y evitar captchas)")
    print("    Recomendado si usas VPN, proxy, o tienes IP din√°mica")
    usar_rotacion = input("    s/n (Enter = no): ").strip().lower() == 's'
    
    vpn_provider = None
    if usar_rotacion:
        print(f"\n[OK] Rotaci√≥n de IP activada (cada {PETICIONES_ANTES_CAMBIO_IP} peticiones)")
        
        from idealista_scraper import IdealistaScraper
        scraper_temp = IdealistaScraper()
        vpns_detectadas = scraper_temp.detectar_vpn_instalada()
        
        print("\n[?] ¬øQuieres cambio de VPN AUTOM√ÅTICO?")
        print("    (Requiere tener una VPN con CLI instalada)")
        print("")
        
        if vpns_detectadas:
            print(f"    ‚úÖ VPNs detectadas en tu sistema: {', '.join(vpns_detectadas)}")
        else:
            print("    ‚ö†Ô∏è No se detectaron VPNs con CLI instaladas")
            print("    (NordVPN, ExpressVPN, ProtonVPN, Surfshark, Windscribe)")
        
        print("")
        print("    Opciones:")
        print("    1. NordVPN (autom√°tico)")
        print("    2. ExpressVPN (autom√°tico)")
        print("    3. ProtonVPN (autom√°tico)")
        print("    4. Surfshark (autom√°tico)")
        print("    5. Windscribe (autom√°tico)")
        print("    6. Manual (te avisar√° para cambiar t√∫)")
        print("")
        
        vpn_opcion = input("    Elige (1-6, Enter = manual): ").strip()
        
        vpn_map = {
            '1': 'nordvpn',
            '2': 'expressvpn', 
            '3': 'protonvpn',
            '4': 'surfshark',
            '5': 'windscribe',
            '6': 'manual'
        }
        
        vpn_provider = vpn_map.get(vpn_opcion, 'manual')
        
        if vpn_provider != 'manual':
            print(f"\n[OK] VPN autom√°tica configurada: {vpn_provider}")
            print("    El scraper cambiar√° de servidor autom√°ticamente")
        else:
            print("\n[OK] Modo manual: te avisar√° cuando debas cambiar IP")
    
    # ============== MODO BATCH O INDIVIDUAL ==============
    
    if is_batch and portal_seleccionado == 'idealista':
        # Batch mode para Idealista via CDP
        scrapear_idealista_batch(seleccion_url, debug, usar_rotacion, vpn_provider, num_paginas)
        print("\n‚úÖ Scraping completado!")
        print("\n[!] El navegador Chrome sigue abierto. NO lo cierres si quieres seguir us√°ndolo.")
        return
    
    # ============== MODO INDIVIDUAL (compatibilidad) ==============
    
    if is_batch:
        urls_list = seleccion_url
    else:
        urls_list = [seleccion_url]
    
    search_url = urls_list[0]['url']
    nombre = urls_list[0]['nombre']
    
    try:
        scraper = ScraperFactory.create_scraper(
            portal_seleccionado,
            modo_debug=debug,
            usar_rotacion_ip=usar_rotacion,
            vpn_provider=vpn_provider,
            search_url=search_url
        )
    except ValueError as e:
        print(f"\n[ERROR] {e}")
        return
    
    # ============== CONECTAR CHROME ==============
    
    if not scraper.conectar_chrome():
        return
    
    # ============== NAVEGACI√ìN Y SCRAPING ==============
    
    print("\n[?] OPCIONES:")
    print("    1. Navegar autom√°ticamente a la URL y scrapear")
    print("    2. Ya estoy en la p√°gina, scrapear directamente")
    
    opcion = input("\nElige (1 o 2): ").strip()
    
    if opcion == "1":
        scraper.navegar_a_url()
    
    # Scrapear
    print(f"\n[*] Iniciando scraping de {info_portal['name']}...")
    viviendas = scraper.scrapear_con_filtrado(num_paginas, ubicacion=nombre)
    
    if not viviendas:
        print("\n[!] No se encontraron viviendas de particulares")
        return
    
    # ============== GUARDAR Y MOSTRAR RESULTADOS ==============
    
    portal_name = info_portal['name'].lower().replace(' ', '_')
    filename = f"viviendas_{portal_name}_{nombre.replace(' ', '_').replace('/', '-')}.json"
    scraper.guardar(viviendas, filename, ubicacion=nombre, url_scrapeada=search_url)
    
    scraper.mostrar_resumen(viviendas)
    
    print("\n[OK] Scraping completado!")
    print(f"[OK] Archivo: {filename}")
    print("\n[!] El navegador Chrome sigue abierto. NO lo cierres si quieres seguir us√°ndolo.")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n[!] Interrumpido por el usuario")
    except Exception as e:
        print(f"\n[ERROR] {e}")
        import traceback
        traceback.print_exc()
